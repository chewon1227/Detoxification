import json, uuid, os, torch
from typing import List, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import BitsAndBytesConfig
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance
from peft import PeftModel
# import openai
from dotenv import load_dotenv
# from google.colab import userdata

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Device: {device}")


# ### DB êµ¬ì¶•í•˜ê¸°

# In[3]:


# embedding_model = SentenceTransformer('dragonkue/BGE-m3-ko')
embedding_model = SentenceTransformer('/tmp/BGE-m3-ko')

# In[ ]:

DB_PATH = "qdrant_bge"
COLLECTION_NAME = "dcinside"

# ì¸ë±ìŠ¤ ìƒì„±í•˜ê¸°
def database_indexing(client):
    data_path = "merged_dataset.json"

    with open(data_path, 'r', encoding='utf-8') as f:
        dataset = json.load(f)

    points_to_upsert = []
    batch_size = 100

    for idx, item in enumerate(dataset):
        main_text = item.get('main', '')
        comments = item.get('comments', [])
        comments_text = ' '.join(comments) if comments else ''

        full_text = f"{main_text} {comments_text}".strip()

        if not full_text:
            continue

        vector = embedding_model.encode(full_text).tolist()

        payload = {
            "date": item.get('date', ''),
            "main": main_text,
            "comments": comments,
            "source_url": item.get('source_url', ''),
            "gallery": item.get('gallery', ''),
            "full_text": full_text
        }

        point = PointStruct(
            id=str(uuid.uuid4()),
            vector=vector,
            payload=payload
        )
        points_to_upsert.append(point)

        if len(points_to_upsert) >= batch_size:
            client.upsert(
                collection_name=COLLECTION_NAME,
                points=points_to_upsert,
                wait=True
            )
            print(f"Indexed {idx + 1}/{len(dataset)} items...")
            points_to_upsert = []

    if points_to_upsert:
        client.upsert(
            collection_name=COLLECTION_NAME,
            points=points_to_upsert,
            wait=True
        )

    print(f"index completed : {len(dataset)}")

    return client


def database_check():
    client = QdrantClient(path=DB_PATH)

    collections = client.get_collections()
    collection_names = [c.name for c in collections.collections]

    if COLLECTION_NAME in collection_names:
        print(f" existing collection '{COLLECTION_NAME}'")
    else:
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
        )
        print(f" new collection '{COLLECTION_NAME}' ")
        client = database_indexing(client)
    return client





# In[ ]:


# openai.api_key = userdata.get('OPENAI_KEY')


# ### RAG í•¨ìˆ˜ ì •ì˜

# In[8]:


def search_documents(client, query: str, top_k: int = 3, gallery_filter: str = None) -> List[Dict]:
    query_vector = embedding_model.encode(query).tolist()

    query_filter = None
    if gallery_filter:
        from qdrant_client.models import Filter, FieldCondition, MatchValue
        query_filter = Filter(
            must=[
                FieldCondition(
                    key="gallery",
                    match=MatchValue(value=gallery_filter)
                )
            ]
        )

    search_response = client.query_points(
        collection_name=COLLECTION_NAME,
        query=query_vector,
        query_filter=query_filter,
        limit=top_k,
        with_payload=True
    )

    results = []
    for hit in search_response.points:
        results.append({
            "score": hit.score,
            "date": hit.payload.get('date', ''),
            "main": hit.payload.get('main', ''),
            "comments": hit.payload.get('comments', []),
            "source_url": hit.payload.get('source_url', ''),
            "gallery": hit.payload.get('gallery', ''),
            "full_text": hit.payload.get('full_text', '')
        })

    return results


# In[9]:


def _prepare_rag_context(client, query: str, top_k: int = 3, gallery_filter: str = None) -> tuple:
    retrieved_docs = search_documents(client, query, top_k=top_k, gallery_filter=gallery_filter)

    context_parts = []
    for i, doc in enumerate(retrieved_docs, 1):
        context_parts.append(f"[ë¬¸ì„œ {i}]")
        context_parts.append(f"ë‚ ì§œ: {doc['date']}")
        context_parts.append(f"ê°¤ëŸ¬ë¦¬: {doc['gallery']}")
        context_parts.append(f"ë‚´ìš©: {doc['main']}")
        if doc['comments']:
            context_parts.append(f"ëŒ“ê¸€: {', '.join(doc['comments'][:3])}")
        context_parts.append("")

    context = "\n".join(context_parts)
    return retrieved_docs, context


# apië¡œ ë°ë ¤ì˜¤ê¸°

# In[10]:


def generate_rag_response_api(
    query: str,
    model: str = "gpt-4o-mini",
    top_k: int = 3,
    gallery_filter: str = None,
    max_tokens: int = 512,
    temperature: float = 0.7
) -> Dict[str, Any]:

    retrieved_docs, context = _prepare_rag_context(query, top_k, gallery_filter)
    prompt = f"""ë‹¤ìŒ ë¬¸ì„œì— ìˆëŠ”ëŒ€ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""

    response = openai.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œì— ìˆëŠ”ëŒ€ë¡œ ë‹µë³€í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=max_tokens,
        temperature=temperature
    )
    answer = response.choices[0].message.content.strip()

    return {
        "query": query,
        "answer": answer,
        "retrieved_docs": retrieved_docs,
        "model_used": model
    }


# In[ ]:


# get_ipython().system('ls -lh ~/.cache/huggingface/hub | grep models')


# ë¡œì»¬ ëª¨ë¸ë¡œ ë°ë ¤ì˜¤ê¸°

# In[14]:

def model_setup(mode):
    load_dotenv()
    model_name = os.getenv("BASE_MODEL_NAME")
    if mode == "detox": 
        adapter_name = os.getenv("DETOX_ADAPTER_NAME")
    
    print(model_name)

    # ì–‘ìí™” í•„ìš”í•  ì‹œ (ì½”ë© ê¸°ì¤€ 8B ì´ìƒ)
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quantization_config, # í•„ìš”í•  ì‹œ
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto" if device == "cuda" else None,
        low_cpu_mem_usage=True
    )

    if mode == "detox":
        model = PeftModel.from_pretrained(model, adapter_name)
    

    print('load done !')
    
    return tokenizer, model


# In[ ]:
import re
import torch
import gc

def generate_rag_response_local(
    tokenizer,
    model,
    client,
    query: str,
    summary: str,
    top_k: int = 10,
    gallery_filter: str = None,
    max_tokens: int = 150,
    temperature: float = 0.55
) -> Dict[str, Any]:

    # 1. ë©”ëª¨ë¦¬ ì²­ì†Œ
    gc.collect()
    torch.cuda.empty_cache()

    # 2. ë¬¸ì„œ ê²€ìƒ‰
    retrieved_docs, _ = _prepare_rag_context(client,query, top_k, gallery_filter)

    unique_docs = []
    seen_contents = set()

    # 3. ë°ì´í„° ì •ì œ
    for doc in retrieved_docs:
        main_text = doc['main'].strip()
        if len(main_text) < 15: continue # ë„ˆë¬´ ì§§ì€ ë…¸ì´ì¦ˆ ì œê±°

        content_parts = [main_text]
        if doc['comments']:
            content_parts.extend(doc['comments'][:2])

        full_content = " ".join(content_parts).replace("\n", " ")

        if full_content not in seen_contents:
            unique_docs.append(doc)
            seen_contents.add(full_content)

    # 4. Context êµ¬ì„±
    context_list = []
    total_length = 0
    MAX_CONTEXT_LENGTH = 1500

    for doc in unique_docs:
        text = f"{doc['main']} {(' '.join(doc['comments'][:1]))}"
        if total_length + len(text) > MAX_CONTEXT_LENGTH: break
        context_list.append(text)
        total_length += len(text)

    context = "\n".join(context_list)

    print(f"\nğŸ” ì›ë³¸: {len(retrieved_docs)}ê°œ -> âœ¨ ì •ì œ í›„: {len(context_list)}ê°œ")
    print("\n" + "="*20 + " [Target Context] " + "="*20)
    print(context[:300] + "..." if len(context) > 300 else context)
    print("="*60 + "\n")

    if not context.strip():
        return {"query": query, "answer": "ê´€ë ¨ ë–¡ë°¥ ì—†ì–´ì„œ ëª¨ë¦„.", "retrieved_docs": []}

    # 5. í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ë¬¸ë§¥ ì •ë¦¬ ì§€ì‹œ ì¶”ê°€)
    system_prompt = f"""
    ë„ˆëŠ” 'ë””ì‹œì¸ì‚¬ì´ë“œ' ê°¤ëŸ¬ë¦¬ ìœ ì €ë‹¤.
    {summary}
    ì£¼ì–´ì§„ [í…ìŠ¤íŠ¸] ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ë°˜ë§(ë¹„ì†ì–´, ìŒìŠ´ì²´)**ë¡œ ìœ„ [ê²Œì‹œê¸€]ì— ëŒ“ê¸€ì„ ë‹¬ì•„ë¼.

    [ê·œì¹™]
    1. **ì •ë¦¬í•´ë¼:** [í…ìŠ¤íŠ¸]ê°€ ë‘ì„œì—†ìœ¼ë©´, í•µì‹¬ë§Œ ë½‘ì•„ì„œ ìì—°ìŠ¤ëŸ¬ìš´ í•œ ë¬¸ì¥ìœ¼ë¡œ ì—°ê²°í•´ë¼.
    2. **ë§íˆ¬:** "~ì„", "~í•¨", "ã…‹ã…‹", "ã„¹ã…‡", "ã…‚ã……" ë“±ì„ ì¨ì„œ ê±°ì¹ ê²Œ ë§í•´ë¼.
    3. **ë‚ ì¡° ê¸ˆì§€:** ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ˆë¼.
    """

    messages = [
        {
            "role": "system",
            "content": system_prompt
        },
        {
            "role": "user",
            "content": (
                f"ì•„ë˜ [í…ìŠ¤íŠ¸] ì½ê³  ë””ì‹œ ë§íˆ¬ë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´ì„œ ìœ„ [ê²Œì‹œê¸€] ë°˜ë°•í•´.\n"
                f"ì—‰ëš±í•œ ì†Œë¦¬ í•˜ì§€ ë§ê³  í•µì‹¬ë§Œ ì°”ëŸ¬.\n\n"
                f"[í…ìŠ¤íŠ¸]\n{context}\n\n"
                f"ëŒ“ê¸€:"
            )
        }
    ]

    print("final prompt\n")
    print(messages)

    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(device)

    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]
    terminators = [t for t in terminators if t is not None]

    model.eval()
    torch.set_grad_enabled(False)

    # 6. ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            input_ids,
            max_new_tokens=max_tokens,
            eos_token_id=terminators,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,
            temperature=temperature,
            top_p=0.9,
            repetition_penalty=1.1,
            no_repeat_ngram_size=0,
        )

    # 7. ë‹µë³€ ì¶”ì¶œ
    response = outputs[0][input_ids.shape[-1]:]
    answer = tokenizer.decode(response, skip_special_tokens=True).strip()

    if "ë³€í™˜ ê²°ê³¼:" in answer: answer = answer.split("ë³€í™˜ ê²°ê³¼:")[-1].strip()

    # ë¬¸ì¥ ì™„ê²°ì„± ì²´í¬ (ì˜ë¦¼ ë°©ì§€)
    valid_endings = ['.', '!', '?', 'ã…‹', 'ã…', '~', 'ë‹¤', 'ìš”', 'ì„', 'í•¨', 'ìŒ', 'ë„¤', 'ëƒ', 'ë…¸']
    if len(answer) > 0 and answer[-1] not in valid_endings:
        last_valid = -1
        for i in range(len(answer)-1, -1, -1):
            if answer[i] in valid_endings or answer[i] == ' ':
                last_valid = i
                break
        if last_valid != -1: answer = answer[:last_valid+1]
        else: answer += "..."

    # [í•µì‹¬ ìˆ˜ì •] ì •ê·œì‹ ë²”ìœ„ í™•ì¥ (ììŒ/ëª¨ìŒ í—ˆìš©)
    # ã„±-ã… (ììŒ), ã…-ã…£ (ëª¨ìŒ)ì„ ì¶”ê°€í•˜ì—¬ 'ã…‚ã……', 'ã„´ã„´' ë“±ì´ ì§€ì›Œì§€ì§€ ì•Šê²Œ í•¨
    answer = re.sub(r'[^ê°€-í£ã„±-ã…ã…-ã…£0-9\s.,!?~]', '', answer)
    answer = re.sub(r'\s+', ' ', answer).strip()

    del input_ids, outputs
    torch.cuda.empty_cache()

    return {
        "query": query,
        "answer": answer,
        "retrieved_docs": unique_docs
    }

def generate_rag_response_local_old(
    tokenizer,
    model,
    client,
    query: str,
    top_k: int = 3,
    gallery_filter: str = None,
    max_tokens: int = 200,
    temperature: float = 0.2,
) -> Dict[str, Any]:

    retrieved_docs, context = _prepare_rag_context(client, query, top_k, gallery_filter)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # Prompt êµ¬ì„±
    prompt = f"""
#Constraints. 
##1. Answer the question based ONLY on the provided Documents below.
##2. Do not make up information.
##3. you MUST answer in **KOREAN**.
##4. Output a string of the following format: 
    - "your answer ONLY"
- ex1) 
    "ì—¬ê°€ë¶€ê°™ì€ ì†Œë¦¬í•˜ê³  ì•‰ì•˜ë…¸ã…‹ã…‹ ì ˆëŒ€ ì•ˆëœë‹¤ ê²Œì´ì•¼ã…‹ã…‹ã…‹ã…‹ã…‹"
- ex2)
    "ëª¨ë³‘ì œëŠ” ì”¨ë°œ ì¢†ê°™ì€ ì†Œë¦¬í•˜ê³  ì•‰ì•˜ë„¤ ã„¹ã…‡"


#Documents:
-{context}

#Question: 
-{query}

Answer:
  """
    print("\n")
    print("====final prompt====")
    print(prompt)

    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            do_sample=True,
            top_p=0.8,
            pad_token_id=tokenizer.eos_token_id,
            repetition_penalty = 1.3
        )

    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    answer = generated_text.split("Answer:")[-1].strip()

    return answer


# í…ŒìŠ¤íŠ¸ 1. ì§ˆë¬¸ì— ë‹µí•´ë³´ê±°ë¼



# In[ ]:


def interactive_rag():
    while True:
        user_input = input("\nì§ˆë¬¸: ").strip()
        if user_input.lower() in ['quit', 'exit']:
            print("\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break


# In[ ]:


