{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uin5h9OfikdJ"
      },
      "outputs": [],
      "source": [
        "!pip install qdrant-client sentence-transformers transformers torch accelerate openai bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI2ncBIPilmd",
        "outputId": "1b6d1758-5373-44d9-ecd2-0398500b6815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import json, uuid, os, torch, openai, re \n",
        "from typing import List, Dict, Any\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
        "from google.colab import userdata\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kgtk0VljO3H"
      },
      "source": [
        "### DB êµ¬ì¶•í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiYhTzmeis4o"
      },
      "outputs": [],
      "source": [
        "embedding_model = SentenceTransformer('dragonkue/BGE-m3-ko')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y11UDuTo7h98",
        "outputId": "b90bd666-1aca-40ba-a97d-fc0cf8c708b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " existing collection 'dcinside'\n"
          ]
        }
      ],
      "source": [
        "DB_PATH = \"/content/drive/MyDrive/yonsei/YAI/qdrant_bge\"\n",
        "COLLECTION_NAME = \"dcinside\"\n",
        "\n",
        "client = QdrantClient(path=DB_PATH)\n",
        "\n",
        "collections = client.get_collections()\n",
        "collection_names = [c.name for c in collections.collections]\n",
        "\n",
        "if COLLECTION_NAME in collection_names:\n",
        "    print(f\" existing collection '{COLLECTION_NAME}'\")\n",
        "else:\n",
        "    client.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=VectorParams(size=1024, distance=Distance.COSINE)\n",
        "    )\n",
        "    print(f\" new collection '{COLLECTION_NAME}' \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcfGlMuW8QhF"
      },
      "source": [
        "ì¸ë±ìŠ¤ ìƒì„±í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxN7KyQL8QWv"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/drive/MyDrive/yonsei/YAI/merged_dataset.json\"\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "points_to_upsert = []\n",
        "batch_size = 100\n",
        "\n",
        "for idx, item in enumerate(dataset):\n",
        "    main_text = item.get('main', '')\n",
        "    comments = item.get('comments', [])\n",
        "    comments_text = ' '.join(comments) if comments else ''\n",
        "\n",
        "    full_text = f\"{main_text} {comments_text}\".strip()\n",
        "\n",
        "    if not full_text:\n",
        "        continue\n",
        "\n",
        "    vector = embedding_model.encode(full_text).tolist()\n",
        "\n",
        "    payload = {\n",
        "        \"date\": item.get('date', ''),\n",
        "        \"main\": main_text,\n",
        "        \"comments\": comments,\n",
        "        \"source_url\": item.get('source_url', ''),\n",
        "        \"gallery\": item.get('gallery', ''),\n",
        "        \"full_text\": full_text\n",
        "    }\n",
        "\n",
        "    point = PointStruct(\n",
        "        id=str(uuid.uuid4()),\n",
        "        vector=vector,\n",
        "        payload=payload\n",
        "    )\n",
        "    points_to_upsert.append(point)\n",
        "\n",
        "    if len(points_to_upsert) >= batch_size:\n",
        "        client.upsert(\n",
        "            collection_name=COLLECTION_NAME,\n",
        "            points=points_to_upsert,\n",
        "            wait=True\n",
        "        )\n",
        "        print(f\"Indexed {idx + 1}/{len(dataset)} items...\")\n",
        "        points_to_upsert = []\n",
        "\n",
        "if points_to_upsert:\n",
        "    client.upsert(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        points=points_to_upsert,\n",
        "        wait=True\n",
        "    )\n",
        "\n",
        "print(f\"index completed : {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nm3rL-SfrOoC"
      },
      "outputs": [],
      "source": [
        "openai.api_key = userdata.get('OPENAI_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N7MKo0XkEh8"
      },
      "source": [
        "### RAG í•¨ìˆ˜ ì •ì˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mmCzdrfwkFS-"
      },
      "outputs": [],
      "source": [
        "def search_documents(query: str, top_k: int = 3, gallery_filter: str = None) -> List[Dict]:\n",
        "    query_vector = embedding_model.encode(query).tolist()\n",
        "\n",
        "    query_filter = None\n",
        "    if gallery_filter:\n",
        "        from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
        "        query_filter = Filter(\n",
        "            must=[\n",
        "                FieldCondition(\n",
        "                    key=\"gallery\",\n",
        "                    match=MatchValue(value=gallery_filter)\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    search_response = client.query_points(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        query=query_vector,\n",
        "        query_filter=query_filter,\n",
        "        limit=top_k,\n",
        "        with_payload=True\n",
        "    )\n",
        "\n",
        "    results = []\n",
        "    for hit in search_response.points:\n",
        "        results.append({\n",
        "            \"score\": hit.score,\n",
        "            \"date\": hit.payload.get('date', ''),\n",
        "            \"main\": hit.payload.get('main', ''),\n",
        "            \"comments\": hit.payload.get('comments', []),\n",
        "            \"source_url\": hit.payload.get('source_url', ''),\n",
        "            \"gallery\": hit.payload.get('gallery', ''),\n",
        "            \"full_text\": hit.payload.get('full_text', '')\n",
        "        })\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "K9_9eORkkKOh"
      },
      "outputs": [],
      "source": [
        "def _prepare_rag_context(query: str, top_k: int = 3, gallery_filter: str = None) -> tuple:\n",
        "    retrieved_docs = search_documents(query, top_k=top_k, gallery_filter=gallery_filter)\n",
        "\n",
        "    context_parts = []\n",
        "    for i, doc in enumerate(retrieved_docs, 1):\n",
        "        context_parts.append(f\"[ë¬¸ì„œ {i}]\")\n",
        "        context_parts.append(f\"ë‚ ì§œ: {doc['date']}\")\n",
        "        context_parts.append(f\"ê°¤ëŸ¬ë¦¬: {doc['gallery']}\")\n",
        "        context_parts.append(f\"ë‚´ìš©: {doc['main']}\")\n",
        "        if doc['comments']:\n",
        "            context_parts.append(f\"ëŒ“ê¸€: {', '.join(doc['comments'][:3])}\")\n",
        "        context_parts.append(\"\")\n",
        "\n",
        "    context = \"\\n\".join(context_parts)\n",
        "    return retrieved_docs, context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md3Ie1TnCq-j"
      },
      "source": [
        "apië¡œ ë°ë ¤ì˜¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2IZKNCKAYmA"
      },
      "outputs": [],
      "source": [
        "def generate_rag_response_api(\n",
        "    query: str,\n",
        "    model: str = \"gpt-4o-mini\",\n",
        "    top_k: int = 3,\n",
        "    gallery_filter: str = None,\n",
        "    max_tokens: int = 512,\n",
        "    temperature: float = 0.7\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    retrieved_docs, context = _prepare_rag_context(query, top_k, gallery_filter)\n",
        "    prompt = f\"\"\"ë‹¤ìŒ ë¬¸ì„œì— ìˆëŠ”ëŒ€ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
        "\n",
        "{context}\n",
        "\n",
        "ì§ˆë¬¸: {query}\n",
        "\n",
        "ë‹µë³€:\"\"\"\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œì— ìˆëŠ”ëŒ€ë¡œ ë‹µë³€í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    answer = response.choices[0].message.content.strip()\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": answer,\n",
        "        \"retrieved_docs\": retrieved_docs,\n",
        "        \"model_used\": model\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEKp2eY-YlDZ",
        "outputId": "40d0ed7f-fef5-45cf-c262-f6b1ac7683e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "drwxr-xr-x 6 root root 4.0K Nov 21 05:35 models--dragonkue--BGE-m3-ko\n",
            "drwxr-xr-x 6 root root 4.0K Nov 21 07:47 models--naver-hyperclovax--HyperCLOVAX-SEED-Text-Instruct-1.5B\n",
            "drwxr-xr-x 6 root root 4.0K Nov 21 06:20 models--QuixiAI--WizardLM-7B-Uncensored\n"
          ]
        }
      ],
      "source": [
        "!ls -lh ~/.cache/huggingface/hub | grep models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq70lY6pCtPY"
      },
      "source": [
        "ë¡œì»¬ ëª¨ë¸ë¡œ ë°ë ¤ì˜¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfccc86d"
      },
      "outputs": [],
      "source": [
        "# model_name = \"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B\"\n",
        "# model_name = \"EleutherAI/polyglot-ko-5.8b\"\n",
        "# model_name = \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\"\n",
        "# model_name = \"QuixiAI/WizardLM-7B-Uncensored\" í•œêµ­ì–´ë¥¼ ëª»í•¨\n",
        "# model_name = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\" ì½”ë©ì—ì„œ ì•ˆëŒì•„ê° (ì–‘ìí™” í•„ìš”)\n",
        "# model_name = \"Qwen/Qwen2.5-3B\"\n",
        "# model_name = \"chuanli11/Llama-3.2-3B-Instruct-uncensored\"\n",
        "# model_name = \"bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF\"\n",
        "model_name = \"s5ya/Ko-Llama-3.1-8B-Lexi-Uncensored-V2\"\n",
        "\n",
        "# ì–‘ìí™” í•„ìš”í•  ì‹œ (ì½”ë© ê¸°ì¤€ 8B ì´ìƒ)\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config, # í•„ìš”í•  ì‹œ\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "print('load done !')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "lbS3j-JYZzRQ"
      },
      "outputs": [],
      "source": [
        "def generate_rag_response_local(\n",
        "    query: str,\n",
        "    top_k: int = 10, # [ìˆ˜ì •] 5 -> 10 (ë” ë§ì´ ê¸ì–´ì˜´)\n",
        "    gallery_filter: str = None,\n",
        "    max_tokens: int = 150, \n",
        "    temperature: float = 0.45 \n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    # 1. ë¬¸ì„œ ê²€ìƒ‰\n",
        "    retrieved_docs, _ = _prepare_rag_context(query, top_k, gallery_filter)\n",
        "\n",
        "    # [ë””ë²„ê¹…] ê²€ìƒ‰ëœ ì›ë³¸ ê°œìˆ˜ ì¶œë ¥\n",
        "    print(f\"\\nğŸ” ê²€ìƒ‰ëœ ì›ë³¸ ë¬¸ì„œ ê°œìˆ˜: {len(retrieved_docs)}ê°œ\")\n",
        "\n",
        "    # 2. ì¤‘ë³µ ì œê±° ë° ë‚´ìš© ë³‘í•©\n",
        "    unique_docs = []\n",
        "    seen_contents = set()\n",
        "    \n",
        "    for doc in retrieved_docs:\n",
        "        # ë³¸ë¬¸ê³¼ ëŒ“ê¸€ì„ ëª¨ë‘ í•©ì³ì„œ í’ë¶€í•œ Context ë§Œë“¤ê¸°\n",
        "        content_parts = [doc['main'].strip()]\n",
        "        if doc['comments']:\n",
        "            # ëŒ“ê¸€ë„ ë‚´ìš©ì— í¬í•¨ (ë„ˆë¬´ ì§§ì€ ë³¸ë¬¸ ë³´ì™„)\n",
        "            content_parts.extend(doc['comments'][:2])\n",
        "        \n",
        "        full_content = \" \".join(content_parts)\n",
        "        \n",
        "        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì¤‘ë³µì´ë©´ íŒ¨ìŠ¤, ì•„ë‹ˆë©´ ì¶”ê°€\n",
        "        if len(full_content) > 2 and full_content not in seen_contents:\n",
        "            unique_docs.append(doc)\n",
        "            seen_contents.add(full_content)\n",
        "\n",
        "    # [ë””ë²„ê¹…] ì¤‘ë³µ ì œê±° í›„ ê°œìˆ˜ ì¶œë ¥\n",
        "    print(f\"âœ¨ ì¤‘ë³µ ì œê±° í›„ ë‚¨ì€ ë¬¸ì„œ: {len(unique_docs)}ê°œ\")\n",
        "    \n",
        "    # Context êµ¬ì„±\n",
        "    context_list = []\n",
        "    for doc in unique_docs:\n",
        "        # ë³¸ë¬¸ + ëŒ“ê¸€ ê²°í•©\n",
        "        text = doc['main']\n",
        "        if doc['comments']:\n",
        "            text += \" (\" + \" \".join(doc['comments'][:2]) + \")\"\n",
        "        context_list.append(text)\n",
        "        \n",
        "    context = \"\\n\".join(context_list)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*20 + \" [Context Preview] \" + \"=\"*20)\n",
        "    print(context[:500] + \"...\" if len(context) > 500 else context)\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ë°”ë¡œ ë¦¬í„´\n",
        "    if not context.strip():\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"answer\": \"ê´€ë ¨ëœ ë‚´ìš©ì´ DBì— ì—†ìŒ.\",\n",
        "            \"retrieved_docs\": []\n",
        "        }\n",
        "\n",
        "    # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ë¶€ì • ëª…ë ¹ ì œê±° -> ì–‘ì„± ëª…ë ¹ ì „í™˜)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"ë„ˆëŠ” ë””ì‹œì¸ì‚¬ì´ë“œ ê°¤ëŸ¬ë¦¬ ìœ ì €ë‹¤. \"\n",
        "                \"ì£¼ì–´ì§„ [í…ìŠ¤íŠ¸]ë“¤ì˜ ë‚´ìš©ì„ ì¢…í•©í•´ì„œ **ë°˜ë§(ë¹„ì†ì–´, ìŒìŠ´ì²´)**ë¡œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ë¼. \"\n",
        "                \"ë‹ˆ ìƒê°ì€ ë„£ì§€ ë§ê³ , í…ìŠ¤íŠ¸ì— ìˆëŠ” ì—¬ë¡ ë§Œ ê·¸ëŒ€ë¡œ ìŠì–´ë¼.\"\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                f\"ì•„ë˜ [í…ìŠ¤íŠ¸] ì½ê³  ë””ì‹œ ë§íˆ¬ë¡œ í•œ ì¤„ ìš”ì•½í•´.\\n\"\n",
        "                f\"ë‚´ìš©:\\n{context}\\n\\n\"\n",
        "                f\"ìš”ì•½:\"\n",
        "            )\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    terminators = [\n",
        "        tokenizer.eos_token_id,\n",
        "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    # 4. ìƒì„±\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_tokens,\n",
        "            eos_token_id=terminators,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1, # 1.1ë¡œ ì•ˆì „í•˜ê²Œ ë³µê·€\n",
        "            no_repeat_ngram_size=0,\n",
        "        )\n",
        "\n",
        "    # 5. ë‹µë³€ ì¶”ì¶œ\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    answer = tokenizer.decode(response, skip_special_tokens=True).strip()\n",
        "\n",
        "    # [í›„ì²˜ë¦¬]\n",
        "    if \"ìš”ì•½:\" in answer:\n",
        "        answer = answer.split(\"ìš”ì•½:\")[-1].strip()\n",
        "    \n",
        "    # ë¶ˆí•„ìš”í•œ ê¸°í˜¸/ì˜ì–´ ì œê±° (í•œê¸€, ìˆ«ì, ë¬¸ì¥ë¶€í˜¸ë§Œ)\n",
        "    answer = re.sub(r'[^ê°€-í£0-9\\s.,!?~ã…‹ã…]', '', answer)\n",
        "    answer = re.sub(r'\\s+', ' ', answer).strip()\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": answer,\n",
        "        \"retrieved_docs\": unique_docs\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oat3N4EQkQ9D"
      },
      "source": [
        "í…ŒìŠ¤íŠ¸ 1. ì§ˆë¬¸ì— ë‹µí•´ë³´ê±°ë¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdHHAiDgAhLe",
        "outputId": "643e3904-f2cd-4727-abef-b2e9011b3213"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'query': 'ì—¬ìë„ êµ°ëŒ€ì— ê°€ì•¼í•œë‹¤ê³  ìƒê°í•´?', 'answer': 'Answer the question based ONLY on the provided documents. \\n  Do not make up information.\\n\\n  Documents:\\n  [ë¬¸ì„œ 1]\\në‚ ì§œ: 2024.05.12 13:19:15\\nê°¤ëŸ¬ë¦¬: \\në‚´ìš©: ì—¬ì„±ì€ êµ°ëŒ€ 3ë…„ ë³´ë‚´ë¼\\nëŒ“ê¸€: ì´, ã…‡ã…ˆ\\n\\n[ë¬¸ì„œ 2]\\në‚ ì§œ: 2025.03.12 23:47:04\\nê°¤ëŸ¬ë¦¬: \\në‚´ìš©: ë‚˜ ì—¬ì”ë° êµ°ëŒ€ê°€ê³ ì‹¶ìŒ\\nëŒ“ê¸€: í†µì¼ë˜ë„ ì—¬ìë„ êµ°ëŒ€ê°€ëŠ” ì œë„ë¡œ ë°”ê¿”ì•¼í•¨\\n\\n[ë¬¸ì„œ 3]\\në‚ ì§œ: 2023.05.12 17:59:50\\nê°¤ëŸ¬ë¦¬: \\në‚´ìš©: ì—¬ìê°€ êµ°ëŒ€ê°€ëŠ”ê±´ ì•  ì•ˆë‚³ì•„ì„œ ê°€ëŠ”ê±°.\\nëŒ“ê¸€: ì²œì¬ì¸ë°?\\n\\n\\n  Question: ì—¬ìë„ êµ°ëŒ€ì— ê°€ì•¼í•œë‹¤ê³  ìƒê°í•´?\\n\\n  Answer in Korean based on the documents: \\n\\n   (no answer)    // if there is no relevant document or clear opinion expressed\\n\\n\\n      (yes/strongly agree)\\n        : \\n\\n\\n       (somehow/agree)\\n         : \\n\\n\\n\\n     (disagree/somewhat disagree)\\n          : \\n\\n\\n\\n\\n     (clearly/dismissively disagree)\\n           : \\n\\n\\n\\n\\nNote:\\n\\n* The response should be indicated by one of the above options only.\\n* If a strong emotion like \"hate\" or \"love\" appears in any comment but it\\'s related to another user and not directly answering your question then ignore that part.\\n\\n\\n\\nHere are my answers:\\n\\n\\n Gallagher doesn\\'t have an official stance on this issue as they don\\'t provide direct opinions through comments.\\n\\n\\nHowever, looking at the available data from the previous posts we can see some users expressing their thoughts about women joining military service. One commenter suggests changing laws so that both men & women could join the army (\"í†µì¼ë˜ë„ ì—¬ìë„ êµ°ëŒ€ê°€ì§€ë„ë¡ changedë¡œ ë°”ê¿”ì•¼ í•¨\").'}\n"
          ]
        }
      ],
      "source": [
        "result = generate_rag_response_local(\"ì—¬ìë„ êµ°ëŒ€ì— ê°€ì•¼í•œë‹¤ê³  ìƒê°í•´?\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9vdfrLXkcbF"
      },
      "outputs": [],
      "source": [
        "def interactive_rag():\n",
        "    while True:\n",
        "        user_input = input(\"\\nì§ˆë¬¸: \").strip()\n",
        "        if user_input.lower() in ['quit', 'exit']:\n",
        "            print(\"\\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q8q0GHJklMX"
      },
      "outputs": [],
      "source": [
        "interactive_rag()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
