{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uin5h9OfikdJ"
      },
      "outputs": [],
      "source": [
        "!pip install qdrant-client sentence-transformers transformers torch accelerate openai bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "wI2ncBIPilmd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b6d1758-5373-44d9-ecd2-0398500b6815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import json, uuid, os, torch\n",
        "from typing import List, Dict, Any\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kgtk0VljO3H"
      },
      "source": [
        "### DB 구축하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiYhTzmeis4o"
      },
      "outputs": [],
      "source": [
        "embedding_model = SentenceTransformer('dragonkue/BGE-m3-ko')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DB_PATH = \"/content/drive/MyDrive/yonsei/YAI/qdrant_bge\"\n",
        "COLLECTION_NAME = \"dcinside\"\n",
        "\n",
        "client = QdrantClient(path=DB_PATH)\n",
        "\n",
        "collections = client.get_collections()\n",
        "collection_names = [c.name for c in collections.collections]\n",
        "\n",
        "if COLLECTION_NAME in collection_names:\n",
        "    print(f\" existing collection '{COLLECTION_NAME}'\")\n",
        "else:\n",
        "    client.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=VectorParams(size=1024, distance=Distance.COSINE)\n",
        "    )\n",
        "    print(f\" new collection '{COLLECTION_NAME}' \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y11UDuTo7h98",
        "outputId": "b90bd666-1aca-40ba-a97d-fc0cf8c708b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " existing collection 'dcinside'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "인덱스 생성하기"
      ],
      "metadata": {
        "id": "gcfGlMuW8QhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/yonsei/YAI/merged_dataset.json\"\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "points_to_upsert = []\n",
        "batch_size = 100\n",
        "\n",
        "for idx, item in enumerate(dataset):\n",
        "    main_text = item.get('main', '')\n",
        "    comments = item.get('comments', [])\n",
        "    comments_text = ' '.join(comments) if comments else ''\n",
        "\n",
        "    full_text = f\"{main_text} {comments_text}\".strip()\n",
        "\n",
        "    if not full_text:\n",
        "        continue\n",
        "\n",
        "    vector = embedding_model.encode(full_text).tolist()\n",
        "\n",
        "    payload = {\n",
        "        \"date\": item.get('date', ''),\n",
        "        \"main\": main_text,\n",
        "        \"comments\": comments,\n",
        "        \"source_url\": item.get('source_url', ''),\n",
        "        \"gallery\": item.get('gallery', ''),\n",
        "        \"full_text\": full_text\n",
        "    }\n",
        "\n",
        "    point = PointStruct(\n",
        "        id=str(uuid.uuid4()),\n",
        "        vector=vector,\n",
        "        payload=payload\n",
        "    )\n",
        "    points_to_upsert.append(point)\n",
        "\n",
        "    if len(points_to_upsert) >= batch_size:\n",
        "        client.upsert(\n",
        "            collection_name=COLLECTION_NAME,\n",
        "            points=points_to_upsert,\n",
        "            wait=True\n",
        "        )\n",
        "        print(f\"Indexed {idx + 1}/{len(dataset)} items...\")\n",
        "        points_to_upsert = []\n",
        "\n",
        "if points_to_upsert:\n",
        "    client.upsert(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        points=points_to_upsert,\n",
        "        wait=True\n",
        "    )\n",
        "\n",
        "print(f\"index completed : {len(dataset)}\")"
      ],
      "metadata": {
        "id": "AxN7KyQL8QWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = userdata.get('OPENAI_KEY')"
      ],
      "metadata": {
        "id": "nm3rL-SfrOoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N7MKo0XkEh8"
      },
      "source": [
        "### RAG 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mmCzdrfwkFS-"
      },
      "outputs": [],
      "source": [
        "def search_documents(query: str, top_k: int = 3, gallery_filter: str = None) -> List[Dict]:\n",
        "    query_vector = embedding_model.encode(query).tolist()\n",
        "\n",
        "    query_filter = None\n",
        "    if gallery_filter:\n",
        "        from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
        "        query_filter = Filter(\n",
        "            must=[\n",
        "                FieldCondition(\n",
        "                    key=\"gallery\",\n",
        "                    match=MatchValue(value=gallery_filter)\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    search_response = client.query_points(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        query=query_vector,\n",
        "        query_filter=query_filter,\n",
        "        limit=top_k,\n",
        "        with_payload=True\n",
        "    )\n",
        "\n",
        "    results = []\n",
        "    for hit in search_response.points:\n",
        "        results.append({\n",
        "            \"score\": hit.score,\n",
        "            \"date\": hit.payload.get('date', ''),\n",
        "            \"main\": hit.payload.get('main', ''),\n",
        "            \"comments\": hit.payload.get('comments', []),\n",
        "            \"source_url\": hit.payload.get('source_url', ''),\n",
        "            \"gallery\": hit.payload.get('gallery', ''),\n",
        "            \"full_text\": hit.payload.get('full_text', '')\n",
        "        })\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "K9_9eORkkKOh"
      },
      "outputs": [],
      "source": [
        "def _prepare_rag_context(query: str, top_k: int = 3, gallery_filter: str = None) -> tuple:\n",
        "    retrieved_docs = search_documents(query, top_k=top_k, gallery_filter=gallery_filter)\n",
        "\n",
        "    context_parts = []\n",
        "    for i, doc in enumerate(retrieved_docs, 1):\n",
        "        context_parts.append(f\"[문서 {i}]\")\n",
        "        context_parts.append(f\"날짜: {doc['date']}\")\n",
        "        context_parts.append(f\"갤러리: {doc['gallery']}\")\n",
        "        context_parts.append(f\"내용: {doc['main']}\")\n",
        "        if doc['comments']:\n",
        "            context_parts.append(f\"댓글: {', '.join(doc['comments'][:3])}\")\n",
        "        context_parts.append(\"\")\n",
        "\n",
        "    context = \"\\n\".join(context_parts)\n",
        "    return retrieved_docs, context"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "api로 데려오기"
      ],
      "metadata": {
        "id": "Md3Ie1TnCq-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_rag_response_api(\n",
        "    query: str,\n",
        "    model: str = \"gpt-4o-mini\",\n",
        "    top_k: int = 3,\n",
        "    gallery_filter: str = None,\n",
        "    max_tokens: int = 512,\n",
        "    temperature: float = 0.7\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    retrieved_docs, context = _prepare_rag_context(query, top_k, gallery_filter)\n",
        "    prompt = f\"\"\"다음 문서에 있는대로 질문에 답변해주세요.\n",
        "\n",
        "{context}\n",
        "\n",
        "질문: {query}\n",
        "\n",
        "답변:\"\"\"\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"당신은 제공된 문서에 있는대로 답변하는 도우미입니다.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    answer = response.choices[0].message.content.strip()\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": answer,\n",
        "        \"retrieved_docs\": retrieved_docs,\n",
        "        \"model_used\": model\n",
        "    }"
      ],
      "metadata": {
        "id": "m2IZKNCKAYmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh ~/.cache/huggingface/hub | grep models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEKp2eY-YlDZ",
        "outputId": "40d0ed7f-fef5-45cf-c262-f6b1ac7683e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drwxr-xr-x 6 root root 4.0K Nov 21 05:35 models--dragonkue--BGE-m3-ko\n",
            "drwxr-xr-x 6 root root 4.0K Nov 21 07:47 models--naver-hyperclovax--HyperCLOVAX-SEED-Text-Instruct-1.5B\n",
            "drwxr-xr-x 6 root root 4.0K Nov 21 06:20 models--QuixiAI--WizardLM-7B-Uncensored\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "로컬 모델로 데려오기"
      ],
      "metadata": {
        "id": "oq70lY6pCtPY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfccc86d"
      },
      "outputs": [],
      "source": [
        "# model_name = \"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B\"\n",
        "# model_name = \"EleutherAI/polyglot-ko-5.8b\"\n",
        "# model_name = \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\"\n",
        "# model_name = \"QuixiAI/WizardLM-7B-Uncensored\" 한국어를 못함\n",
        "# model_name = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\" 코랩에서 안돌아감 (양자화 필요)\n",
        "# model_name = \"Qwen/Qwen2.5-3B\"\n",
        "# model_name = \"chuanli11/Llama-3.2-3B-Instruct-uncensored\"\n",
        "# model_name = \"bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF\"\n",
        "model_name = \"s5ya/Ko-Llama-3.1-8B-Lexi-Uncensored-V2\"\n",
        "\n",
        "# 양자화 필요할 시 (코랩 기준 8B 이상)\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config, # 필요할 시\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "print('load done !')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_rag_response_local(\n",
        "    query: str,\n",
        "    top_k: int = 3,\n",
        "    gallery_filter: str = None,\n",
        "    max_tokens: int = 200,\n",
        "    temperature: float = 0.2\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    retrieved_docs, context = _prepare_rag_context(query, top_k, gallery_filter)\n",
        "\n",
        "    # Prompt 구성\n",
        "    prompt = f\"\"\"Answer the question based ONLY on the provided documents.\n",
        "  Do not make up information.\n",
        "\n",
        "  Documents:\n",
        "  {context}\n",
        "\n",
        "  Question: {query}\n",
        "\n",
        "  Answer:\n",
        "\n",
        "  Answer in Korean based on the documents:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            top_p=0.8,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty = 1.3\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = generated_text.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": answer\n",
        "    }"
      ],
      "metadata": {
        "id": "lbS3j-JYZzRQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oat3N4EQkQ9D"
      },
      "source": [
        "테스트 1. 질문에 답해보거라"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = generate_rag_response_local(\"여자도 군대에 가야한다고 생각해?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdHHAiDgAhLe",
        "outputId": "643e3904-f2cd-4727-abef-b2e9011b3213"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': '여자도 군대에 가야한다고 생각해?', 'answer': 'Answer the question based ONLY on the provided documents. \\n  Do not make up information.\\n\\n  Documents:\\n  [문서 1]\\n날짜: 2024.05.12 13:19:15\\n갤러리: \\n내용: 여성은 군대 3년 보내라\\n댓글: 이, ㅇㅈ\\n\\n[문서 2]\\n날짜: 2025.03.12 23:47:04\\n갤러리: \\n내용: 나 여잔데 군대가고싶음\\n댓글: 통일되도 여자도 군대가는 제도로 바꿔야함\\n\\n[문서 3]\\n날짜: 2023.05.12 17:59:50\\n갤러리: \\n내용: 여자가 군대가는건 애 안낳아서 가는거.\\n댓글: 천재인데?\\n\\n\\n  Question: 여자도 군대에 가야한다고 생각해?\\n\\n  Answer in Korean based on the documents: \\n\\n   (no answer)    // if there is no relevant document or clear opinion expressed\\n\\n\\n      (yes/strongly agree)\\n        : \\n\\n\\n       (somehow/agree)\\n         : \\n\\n\\n\\n     (disagree/somewhat disagree)\\n          : \\n\\n\\n\\n\\n     (clearly/dismissively disagree)\\n           : \\n\\n\\n\\n\\nNote:\\n\\n* The response should be indicated by one of the above options only.\\n* If a strong emotion like \"hate\" or \"love\" appears in any comment but it\\'s related to another user and not directly answering your question then ignore that part.\\n\\n\\n\\nHere are my answers:\\n\\n\\n Gallagher doesn\\'t have an official stance on this issue as they don\\'t provide direct opinions through comments.\\n\\n\\nHowever, looking at the available data from the previous posts we can see some users expressing their thoughts about women joining military service. One commenter suggests changing laws so that both men & women could join the army (\"통일되도 여자도 군대가지도록 changed로 바꿔야 함\").'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9vdfrLXkcbF"
      },
      "outputs": [],
      "source": [
        "def interactive_rag():\n",
        "    while True:\n",
        "        user_input = input(\"\\n질문: \").strip()\n",
        "        if user_input.lower() in ['quit', 'exit']:\n",
        "            print(\"\\n대화를 종료합니다.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q8q0GHJklMX"
      },
      "outputs": [],
      "source": [
        "interactive_rag()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}